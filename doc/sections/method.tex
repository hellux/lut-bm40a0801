\documentclass[doc/report.tex]{subfiles}

% Describe the method which you have chosen for solving the task and explain
% why you have chosen this particular method.

\begin{document}

\section{Method}
A set of different classifiers were implemented using MatLab. In order to find
a good method, multiple methods and approaches were tested. How all of the
methods were implemented is described below in this chapter.

For some algorithms, existing built-in implementations of MatLab were used. In
these cases functions that were used are referenced.


\subsection{Preprocessing}
% TODO

\subsubsection{Data partitioning}
As the given dataset was already split into training and test sets, no further
partitioning was performed. The training set was used for training and the test
set was used for evaluation of the classifier. The number of each class from
each dataset is listed in table \ref{tbl:datasets}.

\begin{table}[h]
    \centering
    \begin{tabular}{l|rr|r}
        class & training & test & all \\\hline
        airplane    & 484   & 243   & 727 \\
        car         & 645   & 323   & 968 \\
        cat         & 590   & 295   & 885 \\
        dog         & 468   & 234   & 702 \\
        flower      & 562   & 281   & 843 \\
        fruit       & 667   & 333   & 1000 \\
        motorbike   & 525   & 263   & 788 \\
        person      & 658   & 328   & 986 \\\hline
        all         & 4599  & 2300  & 6899
    \end{tabular}
    \caption{Distribution of classes in datasets.}
    \label{tbl:datasets}
\end{table}

\subsubsection{Image augmentation}
Image augmentation was tested to see if it has a noticeable impact on the
accuracy of the classifier. Image augmentation was performed by adding random
variations of training samples to the training set using the
\texttt{augmentedImageDatastore}\footnote{https://www.mathworks.com/help/deeplearning/ref/augmentedimagedatastore.html}.
MatLab function. Variations that were used was horizontal reflections and
displacements in X and Y axes.

\subsection{Using Pre-Trained Network}
% TODO intro

\subsubsection{Feature Extraction from Pretrained Networks}
By means of a pre-trained model that has been trained on similar datsets we can
build a very accurate model without the hassle of the training the network in a
timesaving manner. This is accomplished by extracting the image features from
thetrainig set based on the patterns that have been learnt by these pre-trained
network while solving a different problem rather than starting the learning
process from scratch. This way, this method leverage previous learnings and
avoid starting from scratch and saves time.

\subsubsection{Retraining the Pre-Trained AlexNet}
When one is retraining a pre-trained model for their own application, one
starts by removing the original classifier, and adding a new classifier that
fits the task at hand. During the transfer learning implementation the last
three layers of AlexNet were modified to correspond to the objective of the
current classification task. Namely, the number of classes in the Fully
Connected Layer was set to the number of unique classes in the training set.

\subsection{Training Parameter Optimization}
% TODO intro

\subsubsection{Batch Size and Training Epochs Tradeoff}
The size of mini-batches is the frequency of updates: the smaller the
mini-batches the more will be the number of weight updates. As one would expect
a single update with a big mini-batch is more accurate as compared to one with
a small mini-batch. Moreover, bigger batches require lesser updates to reach
the same level of accuracy. However, after the experimental evaluation it was
observed that the smaller batches are preferable. This seems reasonable as
smaller batches would imply smaller steps are taken towards convergence which
in turn would minimize the chances of overshooting the optimal convergence
point.

\subsubsection{Learning rate}
Learning rates of the network are one of the most influential parameters while
training. Deep learning algorithms are trained using a stochastic gradient
descent approaches like Adaptive Moment Optimization (ADAM), Root Mean Square
Propogation (RMSProp), etc. The learning rate tells the optimizer how far to
move the weights in the direction opposite to the gradient for a mini-batch.
Low learning rates are reliable but require a large amount of training time as
the steps needed to converge are very small. Large learning rates on the
contrary are unreliable and generally converge quickly to suboptimal solutions.

One of the commonly used approaches is to start the training with a relatively
large learning rate, since in the beginning random weights are far from
optimal, then periodically reduce the learning rate after a specified number of
epochs. That is why a piecewise learning rate schedule was used during the
training phase.

\subsubsection{ADAM Optimizer}
The ADAM Optimizer was used instead of the stochastic gradient descent (SGD)
for updating the weights of the network, since ADAM is straightforward to
implement, computationally efficient, well suited for problems that are large
in terms of data and parameters and its hyperparameters require little tuning.

\subsection{Feature Extraction}
% TODO intro

\subsubsection{Raw pixels}
One approach that was tested was to simply use raw pixel data as features. It
was done by converting the images to grayscale, resizing them to a fixed size
and concatenating all of the pixel intensities to a feature vector. The images
were resized to a size of 100x100 pixels resulting in 10000 features.

\subsubsection{Local Binary Patterns (LBP)}
Another feature vector that was used was Local Binary Patterns or LBP. It was
computed by resizing each image to e.g. 100x100 pixels and then converting it
into grayscale and dividing it into uniform cells, for example 10x10 pixels for
each cell.

And then for each cell; go through each pixel and calculate a number. The
number is calculated by comparing the 8 neighbouring pixels clockwise to the
pixel value. If the neighbour is larger or equal to the pixel then a 1 is
written to the number, otherwise a zero. The final number is then interpreted
as an 8-bit binary number. When the number for all pixels in the cell has been
computed, a normalized histogram vector is created from the distribution of
numbers within the cell. This is then performed for each cell and the final
result is a concatenation of all created histograms.

To achieve rotation invariance, the histogram bins were changed such that all
non-uniform numbers end up in the same bin, while each uniform number has its
own bin. Uniform numbers are numbers where the binary number transitions from 0
to 1 or 1 to 0 at most 2 times. For example "01000000" is uniform while
"01010101" is not.

\subsubsection{Speeded Up Robust Features (SURF)}
Speeded Up Robust Features or SURF was used as it is often useful for object
recognition.

SURF was extracted from the images by using the
\texttt{bagOfFeatures}\footnote{https://se.mathworks.com/help/vision/ref/bagoffeatures.html}
MatLab function. A bag of features was initially created with the training set.
A histogram of the features for each image was then created with the
\texttt{encode}\footnote{https://se.mathworks.com/help/vision/ref/bagoffeatures.encode.html}
function for both the training set and the test set using the inital bag of
features.

\subsection{Training and classification}

% TODO remaining classifiers? knn, svm, bayesian...
%\subsubsection{Support Vector Machine (SVM)}

\subsubsection{Self-organizing map (SOM)}
Self-organizing maps were considered as they can provide a good visualization
of the classifier which can help to analyze and improve the classification.

Classification with SOM was performed by creating and training a
two-dimensional map using the training dataset according with help of the
MatLab functions
\texttt{selforgmap}\footnote{https://www.mathworks.com/help/deeplearning/ref/selforgmap.html}
and \texttt{train}. As the SOM is typically an unsupervised algorithm the
labels are not used while training the map. The labels are instead used
afterwards to determine what tile is considered what class.

Each tile of the map is considered to belong to the class that has most
classified samples for that tile. When classifying a new sample, it is placed
into a tile by using the SOM and then classified as the class of which that
tile belongs to.

The SOM has one parameter that was adjusted; the dimensions of the map. As the
problem has 8 classes the map must have at least 8 tiles to be able to
differentiate between all classes. But each class could also have more than one
tile as each class may have a set of subclasses. The dimensions was tested and
set as low as possible without losing accuracy as the higher dimension the
higher the training time is.

\end{document}

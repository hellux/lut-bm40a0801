\documentclass[doc/report.tex]{subfiles}

% Present relevant methods and approaches from scientific publications.

\begin{document}

\section{Literature review}

\subsubsection{AlexNet}
AlexNet\cite{Alex}, which has been trained over a million images in the
ImageNet dataset, is one of the most commonly used pretrained network for
transfer learning. It can classify images into 1000 different categories such
as pencil, coffee mug, keyboard etc. and outputs the probability for each of
the object categories. The neural network has about 60 million parameters and
650,000 neurons and consists of five convolutional layers which are followed by
max-pooling layers and three fully-connected layers. To reduce overfitting in
the fully-connected layers the authors had  employed dropout layers in
conjuction with ReLU that proved to be very effective.

A brief descriotion of the fuctions of various AlexNet layers is provided below:

\begin{itemize}
	\item Convolutional Layer puts the images from the previous layer through a set of convolutional filters which extract a particular feature;
	\item ReLU Layer maps negative values to zero which allows faster training while maintaining the positive ones;
	\item Pooling Layer performs downsampling, consequently, reduces the number of CNN parameters;
    	\item Batch Normalization Layer speeds up training by normalizing each of the minibatches;
    	\item Dropout Layer randomly sets the input elements to zero with a given probability and hence protects the network from overfitting;
    	\item Fully Connected Layer identifies the larger patterns by combining all the features learned from the previous layers.
\end{itemize}

\subsubsection{ResNet}
A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex \cite{Res}. They are typically built to overcome the vanishing gradient problem of deep networks wherin the gradient becomes infinitively small due to repeated backpropogations which typically leads to degradation of the perfromance of the network. ResNet utilizes identity shortcut connection that skips one or more layers, or shortcuts to jump over some layers. Typical ResNet models are implemented with double- or triple- layer skips that contain nonlinearities (ReLU) and batch normalization in between

\subsubsection{Transfer Learning}
Until the inception of transfer learning, conventional machine learning
approaches had been traditionally designed to work in isolation. They were
designed to solve a particular task and had to be rebuilt from scratch once the
feature space distribution changed. Transfer learning seeks to break this
notion of task specificity wherein the knowledge acquired from one task could
be utilized to solve another related one. Reusing a pretrained network as a
starting point to learn a new task is a commonly used transfer learning
approach.

An example of the use of transfer learning in image processing was made by
\cite{Ekat}. She had tested two different approaches with AlexNet to identify
Saimaa Ringed Seals from each other. Firstly, it has been used for the task of
segmenting the seal from its background. AlexNet is primarily used for feature
extraction in this case. The second application of AlexNet is in the
reidentification by means of the extarcted image patches of seal patterns. Once
again AlexNet is used for feature extraction. Followed by either retrainig the
exisiting CNN network or by means of an SVM classifier trained on the extracted
features. The results obtained were largely encouraging, about 90 percent of
the pictures were identified correctly. Although, SVM results were slightly
better.
    
\end{document}
